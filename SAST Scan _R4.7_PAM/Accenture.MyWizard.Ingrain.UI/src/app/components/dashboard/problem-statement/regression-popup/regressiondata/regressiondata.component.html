<div class="regression-content">
    
<p align="center"><strong><em><span style="text-decoration: underline;color:darkblue">Machine Learning Model Configuration Details </span></em></strong></p> 
<p><strong><em><span style="text-decoration: underline;">Ridge Regression:</span></em></strong> Ridge regression is the regression type that is intended to be used in cases where the input dataset has the following properties:</p>
<ul>
    <li>Large number of features in consideration</li>
    <li>Problem of collinearity; few of the features are not just correlated to the target variable but also to one another.</li>
    <li>The intention is to penalize certain features instead of completely removing them (unlike Lasso regression)</li>
</ul>
<p>Ridge regression solves the multi-collinearity problem in OLS (Ordinary Least Squared) models by incorporating a shrinkage parameter, &lambda;.</p>
<p>It is seen that as the model complexity increases, the size of the coefficients increases exponentially. Thus, putting constraints on the coefficients can be a good idea. Ridge regression works by penalizing(reducing) the magnitude of coefficients of certain features, while minimizing the MSE (mean squared error). This technique is called regularization. Ridge Regression performs L2 regularization, i.e., it adds a penalty equal to the sum of squares of coefficients to the original linear regression model without any regularization factor.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>alpha </strong>(default value = 0.1): It is the regularization strength. It may either be a positive float or an array of penalties corresponding to specific targets, in which case the size of the array must match the number of targets. It corresponds to the &lambda; parameter.</li>
    <li><strong>copy_X </strong>(<em>optional</em>, default value = True): Should be Boolean; if set to True, it will copy X, otherwise X might be overwritten.</li>
    <li><strong>fit_intercept </strong>(default value = True): It takes a Boolean value as input. If set to True, it will calculate an intercept for the model. Otherwise it will assume that the intercept of the model is zero and hence the model is centered.</li>
    <li><strong>max_iter </strong>(<em>optional</em>): It is the maximum number of iterations for the solver used. It takes integer inputs.</li>
</ul>
<p>&uml;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lsquo;sag&rsquo; solver: max_iter = 1000</p>
<ul>
    <li><strong>solver </strong>(default value = &lsquo;auto&rsquo;): It takes the any of these as inputs - {{ '{' }}&lsquo;auto&rsquo;, &lsquo;svd&rsquo;, &lsquo;cholesky&rsquo;, &lsquo;lsqr&rsquo;, &lsquo;sparse_cg&rsquo;, &lsquo;sag&rsquo;, &lsquo;saga&rsquo;}</li>
    <li><strong>normalize </strong>(<em>optional, </em>default value = False): Takes Boolean values (False by default). This parameter is not taken into account when fit_intercept is set to False. If that&rsquo;s not the case, then X regressors will be normalized by subtracting the mean and dividing by the L2-norm.</li>
    <li><strong>random_state </strong>(<em>optional, </em>default value = None): It specifies the seed for the random number generator which is used to shuffle the data. It takes integer value, by default set to None.</li>
    <li><strong>tol </strong>(default value = 0.001): It specifies the precision of the solution. Takes positive float values.</li>
</ul>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>Ridge_train_score</strong>: The accuracy score achieved when the regression technique is performed on the training data set.</li>
    <li><strong>Ridge_test_score</strong>: The accuracy score achieved when the regression technique is performed on the test data set.</li>
</ul>
<p>&nbsp;</p>
<p><strong><em><span style="text-decoration: underline;">Lasso Regression:</span></em></strong> Very similar to Ridge regression, Lasso regression can be used in cases where the input dataset has the following properties:</p>
<ul>
    <li>Large number of features in the input dataset.</li>
    <li>Issue of multi-collinearity; few of the features are not just correlated to the target variable but also to one another.</li>
    <li>The intention is to so strongly penalize certain features that the algorithm eventually completely removes them.</li>
</ul>
<p>Unlike Ridge Regression, Lasso regression may completely remove the coefficients of a few features in certain range. Also, Lasso Regression performs L1 regularization, i.e., it adds a penalty equal to the sum of absolute value of coefficients to the original linear regression model without any regularization factor.</p>
<p>&nbsp;</p>
<p>Major differences between Lasso and Ridge regression is borne out of the fact that they use L1 and L2 regularization. The keys points to remember are:</p>
<ol>
    <li>Built in feature selection: Since Lasso regression allows the complete removal of useless features by setting their coefficients to zero, it selects important features from the unimportant ones (sparse coefficients). Ridge regression, on the other hand, does non-sparse feature selection but assigning smaller weights to unimportant features.</li>
    <li>Computational Efficiency: L1-norm does not have an analytical solution whereas L2-norm does. This allows ridge regression to be computed more efficiently as compared to Lasso regression.</li>
</ol>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>alpha </strong>(<em>optional, </em>default value = 0.1): It is the regularization strength. It may either be a positive float or an array of penalties corresponding to specific targets, in which case the size of the array must match the number of targets. It corresponds to the &lambda; parameter.</li>
    <li><strong>copy_X </strong>(<em>optional</em>, default value = True): Should be Boolean; if set to True, it will copy X, otherwise X might be overwritten.</li>
    <li><strong>fit_intercept </strong>(<em>optional</em>, default value = True): It takes a Boolean value as input. If set to True, it will calculate an intercept for the model. Otherwise it will assume that the intercept of the model is zero and hence the model is centered.</li>
    <li><strong>max_iter </strong>(<em>optional</em>, default value = 1000): It is the maximum number of iterations for the solver used. It takes integer inputs.</li>
    <li><strong>positive </strong>(<em>optional</em>, default value = True): Takes Boolean values; when set to True, it forces the coefficients to be positive.</li>
    <li><strong>normalize </strong>(<em>optional</em>, default value = False): Takes Boolean values (False by default). This parameter is not taken into account when fit_intercept is set to False. If that&rsquo;s not the case, then X regressors will be normalized by subtracting the mean and dividing by the L2-norm.</li>
    <li><strong>precompute </strong>(default value = False): Takes Boolean values or array as inputs. It specifies whether to use precomputed Gram matrix to speed up calculations. The Gram matrix itself can also be passed as argument. On setting to &lsquo;auto&rsquo; it lets us decide.</li>
    <li><strong>random_state </strong>(<em>optional</em>, default value = None): It specifies the seed for the random number generator which is used to shuffle the data. It takes integer value, by default set to None.</li>
    <li><strong>Selection </strong>(default value = &lsquo;cyclic&rsquo;): Takes a string as input, by default set to &lsquo;cyclic&rsquo;. If set to &lsquo;random&rsquo;, the algorithm will randomly update a coefficient every iteration. This leads to much faster convergence.</li>
    <li><strong>warm_start </strong>(<em>optional</em>, default value = False): It takes Boolean value; if set to False, it will erase the previously found solution, otherwise it&rsquo;ll reuse the previous solution to fit as initialization.</li>
    <li><strong>tol </strong>(<em>optional</em>, default value = 0.001): It specifies the precision of the solution. Takes positive float values.</li>
</ul>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>lasso_train_score</strong>: The accuracy score achieved when the regression technique is performed on the training data set.</li>
    <li><strong>lasso_test_score</strong>: The accuracy score achieved when the regression technique is performed on the test data set.</li>
</ul>
<p><strong><em><span style="text-decoration: underline;">Elastic Net Regression:</span></em></strong> Elastic Net regression combines the best features of both Ridge and Lasso regression by incorporating the penalty terms for both Ridge and Lasso equations in the Elastic Net regression equation.&nbsp; It must also be noted that the lasso regression penalty and the lasso regression penalty get their own &lambda; (shrinkage parameter) values in the final Elastic Net regression equation (&lambda;<sub>1</sub> for lasso regression penalty and &lambda;<sub>2 </sub>for ridge regression penalty).</p>
<p>To find the optimal values for &lambda;<sub>1</sub> and &lambda;<sub>2</sub>, we can perform cross validation on the dataset in hand for different combinations of &lambda;<sub>1</sub> and &lambda;<sub>2</sub>. Making any of the shrinkage parameter equal to zero, we will end up with either Ridge or Lasso regression. And on making both equal to zero, we will have the original Least Squared equation.</p>
<p>Elastic Net Regression turns out to be particularly helpful in situations where there are correlations between parameters.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>alpha </strong>(<em>optional, </em>default value = 0.1): It is the regularization strength. It may either be a positive float or an array of penalties corresponding to specific targets, in which case the size of the array must match the number of targets. It corresponds to the &lambda; parameter.</li>
    <li><strong>l1_ratio </strong>(0 &lt;= value &lt;= 1): It corresponds to a combination of &lambda;<sub>1</sub> and &lambda;<sub>2</sub> parameter explained above. Takes float as inputs. If l1_ratio = 0, it corresponds to &lambda;<sub>2</sub>. And on making l1_ratio = 1, it corresponds to &lambda;<sub>1</sub>.</li>
    <li><strong>copy_X </strong>(<em>optional</em>, default value = True): Should be Boolean; if set to True, it will copy X, otherwise X might be overwritten.</li>
    <li><strong>fit_intercept </strong>(default value = True): It takes a Boolean value as input. If set to True, it will calculate an intercept for the model. Otherwise it will assume that the intercept of the model is zero and hence the model is centered.</li>
    <li><strong>max_iter </strong>(<em>optional</em>, default value = True): It is the maximum number of iterations for the solver used. It takes integer inputs.</li>
    <li><strong>positive </strong>(<em>optional</em>, default value = True): Takes Boolean values; when set to True, it forces the coefficients to be positive.</li>
    <li><strong>normalize </strong>(<em>optional</em>, default value = False): Takes Boolean values (False by default). This parameter is not taken into account when fit_intercept is set to False. If that&rsquo;s not the case, then X regressors will be normalized by subtracting the mean and dividing by the L2-norm.</li>
    <li><strong>precompute </strong>(default value = False): Takes Boolean values or array as inputs. It specifies whether to use precomputed Gram matrix to speed up calculations. The Gram matrix itself can also be passed as argument.</li>
    <li><strong>tol </strong>(<em>optional</em>, default value = 0.001): It specifies the tolerance of the solution. Takes positive float values.</li>
    <li><strong>random_state </strong>(<em>optional</em>, default value = None): It specifies the seed for the random number generator which is used to shuffle the data. It takes integer value, by default set to None.</li>
    <li><strong>selection </strong>(default value = &lsquo;cyclic&rsquo;): Takes a string as input, by default set to &lsquo;cyclic&rsquo;. If set to &lsquo;random&rsquo;, the algorithm will randomly update a coefficient every iteration. This leads to much faster convergence.</li>
    <li><strong>warm_start </strong>(<em>optional</em>, default value = False): It takes Boolean value; if set to False, it will erase the previously found solution, otherwise it&rsquo;ll reuse the previous solution to fit as initialization.</li>
</ul>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>elastic_train_score</strong>: The accuracy score achieved when the regression technique is performed on the training data set.</li>
    <li><strong>elastic_test_score</strong>: The accuracy score achieved when the regression technique is performed on the test data set.</li>
</ul>
<p>&nbsp;</p>
<p><strong><em><span style="text-decoration: underline;">Linear Regression:</span></em></strong> Linear regression is probably the most intuitive and easy to implement regression technique. Linear regression can be univariate (one dependent and one independent variable) or multivariate (two or more dependent, and one independent variable). If the given dataset is such that there exists a linear relationship between the independent variable (the variable that needs to be predicted) and dependent variable(s), then linear regression serves the purpose well. The linear regression model can be represented as</p>
<p>Y = &beta;<sub>0</sub> + &beta;<sub>1</sub>X + &epsilon;</p>
<p>Where Y is the target variable, &beta;<sub>0</sub> is the intercept, &beta;<sub>1</sub> is the coefficient vector for the feature vector X and is the error term.</p>
<p>The main drawbacks of linear regression are as follows:</p>
<ul>
    <li>The final model may give poor prediction results if the dataset suffers from multicollinearity (where there exists a correlation amongst the features in the dataset).</li>
    <li>The issue of autocorrelation (describes the correlation of a variable with itself given a time lag).</li>
    <li>&nbsp;Highly sensitive to outliers. The model thus generated may terribly affect the predicted values.</li>
</ul>
<p>&nbsp;</p>
<p><strong><span style="text-decoration: underline;">Stepwise Regression</span></strong><strong><span style="text-decoration: underline;">:-</span></strong>Stepwise regression is the step-by-step iterative construction of a regression model that involves automatic selection of independent variables.</p>
<p>Most commonly used Stepwise regression methods are listed below:</p>
<ul>
    <li>Standard stepwise regression does two things. It adds and removes predictors as needed for each step.</li>
    <li>Forward selection starts with most significant predictor in the model and adds variable for each step.</li>
    <li>Backward elimination starts with all predictors in the model and removes the least significant variable for each step.</li>
</ul>
<p>The aim of these modelling technique is to maximize the prediction power with minimum number of predictor variables.</p>
<p><strong>Example:- </strong></p>
<p>In the image given below, on the basis of symptoms of Asthama observed in patient we give them advice to either consult a doctor or not on the basis of with minimum number of predictor variables ,if we observe symptoms like SABA,PRN or low dose ICS we don't recommend patient to consult a doctor but if we observe symptoms more than SABA,PRNand low dose ICS (i.e step3,step4,step5,step6) then we recommend to consult a doctor. In this case we don't need more predictive variables on the basis of single predictive variable i.e low dose, medium dose or higher dose we are able to predict the level of Asthama.</p>
<p><strong><img class="stepwise-regression" src="assets/images/regression_stepwiseapproach.png" alt="" /></strong></p>
<p><strong><span style="text-decoration: underline;">Polynomial&nbsp;Regression</span></strong>:-A regression equation is a polynomial regression equation if the power of independent variable is more than 1.</p>
<p>The equation below represents a polynomial equation:- Y= a + b*x^2</p>
<p><strong><span style="text-decoration: underline;">Definition and example</span></strong><span style="text-decoration: underline;">:-</span>The goal of regression analysis is to model the expected value of a dependent variable <em>y</em> in terms of the value of an independent variable (or vector of independent variables) <em>x</em>. In simple linear regression, the model y = &beta; 0 + &beta; 1 x + &epsilon; &nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>is used, where &epsilon; is an unobserved random error with mean zero conditioned on a scalar variable <em>x</em>. In this model, for each unit increase in the value of <em>x</em>, the conditional expectation of <em>y</em> increases by <em>&beta;</em><sub>1</sub> units</p>
<p>While there might be a temptation to fit a higher degree polynomial to get lower error, this can result in over-fitting.</p>
<p><strong><img src="assets/images/regression_ovefitting.png" alt="" /></strong></p>
<p>For example, if we are modeling the yield of a chemical synthesis in terms of the temperature at which the synthesis takes place, we may find that the yield improves by increasing amounts for each unit increase in temperature. In this case, we might propose a quadratic model of the form&nbsp;</p>
<p><sup>y = &beta; </sup><sub>0</sub><sup> + &beta; </sup><sub>1</sub><sup> x + &beta; </sup><sub>2</sub><sup> x 2 + &epsilon;</sup>.</p>
<p>In this model, when the temperature is increased from x to x + 1 units, the expected yield changes by &beta; <sub>1</sub> + &beta; <sub>2</sub> (2 x + 1). (This can be seen by replacing x in this equation with x+1 and subtracting the equation in x from the equation in x+1.) For infinitesimal changes in x, the effect on y is given by the total derivative with respect to x: &beta; <sub>1</sub> + 2 &beta; <sub>2</sub> x.The fact that the change in yield depends on x is what makes the relationship between x and y nonlinear even though the model is linear in the parameters to be estimated.</p>
<p>&nbsp;</p>
<p><strong><em><span style="text-decoration: underline;">Logistic Regression:</span></em></strong>Logistic regression is a variant of Linear Regression which is mainly used for assigning the data-points to various classes/labels. The target variable is categorical in nature and the algorithm uses log of odds as the dependent variable. It uses logit function to predict the probability of occurrence of the categorical event. The favorable class is the one for which the predicted probability is highest amongst all the class. A linear regression model can be represented as</p>
<p>Y = &beta;<sub>0</sub> + &beta;<sub>1</sub>X + &epsilon;</p>
<p>Where Y is the target variable, &beta;<sub>0</sub> is the intercept, &beta;<sub>1</sub> is the coefficient vector for the feature vector X and is the error term.</p>
<p>The sigmoid function used is</p>
<p>p = 1/(1+exp(-Y))</p>
<p>The model thus generated can be evaluated using many methods such as ROC curve (AUC), confusion matrix, Concordance, KS-Statistics etc.</p>
<p>Pros: Easy to implement, straightforward, doesn&rsquo;t require high computation power and easily interpretable, can be updated online.</p>
<p>Cons: Doesn&rsquo;t work well where the features aren&rsquo;t correlated to the target variable and there exists correlation among the feature themselves. The technique is prone to overfitting (which can be somewhat dealt with using regularization). Also, logistic regression fails for non-linear features; in such cases certain transformations are required.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>penalty</strong>: It accepts string values {{'{'}}&lsquo;l1&rsquo; or &lsquo;l2&rsquo;}. By default, &lsquo;l2&rsquo; is taken. It accounts for the penalization norm used.</li>
    <li><strong>dual</strong>: Takes Boolean values, default is False. Describes whether Dual or Primal formulation is used.</li>
    <li><strong>tol</strong>: It specifies the precision of the solution. Takes positive float values.</li>
    <li><strong>C</strong>: It is the inverse of regularization strength. It takes only positive float values.</li>
    <li><strong>fit_intercept</strong>: It takes a Boolean value as input. If set to True, it will calculate an intercept for the model. Otherwise it will assume that the intercept of the model is zero and hence the model is centered.</li>
    <li><strong>intercept_scaling</strong>: Takes float values, default set to 1. A feature with value set equal to intercept_scaling is appended to the instance vector.</li>
    <li><strong>class_weight</strong>: Accepts a dictionary or &lsquo;balanced&rsquo; as inputs. Default is set to None. If not set, all the classes are weighted equally, i.e., their weights are considered equal to 1.</li>
    <li><strong>random_state</strong>: It specifies the seed for the random number generator which is used to shuffle the data. It takes integer value, by default set to None.</li>
    <li><strong>solver</strong>: It takes the any of these as inputs - {{'{'}}&lsquo;newton-cg&rsquo;, &lsquo;lbfgs&rsquo;, &lsquo;liblinear&rsquo;, &lsquo;sag&rsquo;, &lsquo;saga&rsquo;}</li>
    <li><strong>max_iter</strong>: It is the maximum number of iterations for the solver used. It takes integer inputs.</li>
    <li><strong>multi_class</strong>: Takes values in the set {{'{'}}lsquo;ovr&rsquo;, &lsquo;multinomial&rsquo;, &lsquo;auto&rsquo;}; default is &lsquo;ovr&rsquo;. &lsquo;ovr&rsquo; is used for cases where the data is binary in nature. For more than one classes, the multi_class is set to &lsquo;multinomial&rsquo;. On selecting &lsquo;auto&rsquo;, the algorithm selects &lsquo;ovr&rsquo; if it finds the data to be binary, or else, I will choose &lsquo;multinomial&rsquo;.</li>
    <li><strong>verbose</strong>: Takes integer values, default set to 0.</li>
    <li><strong>warm_start</strong>: It takes Boolean value; if set to False, it will erase the previously found solution, otherwise it&rsquo;ll reuse the previous solution to fit as initialization.</li>
    <li><strong>n_jobs</strong>:&nbsp; Takes either an integer value or None as input. It denotes the number of CPU cores used for parallelizing the process of classification.</li>
</ul>
<p><span style="text-decoration: underline;">&nbsp;</span></p>
<p><span style="text-decoration: underline;">OUTPUT:</span></p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>logistic_train_score</strong>: The accuracy score achieved when the regression technique is performed on the training data set.</li>
    <li><strong>logistic_test_score</strong>: The accuracy score achieved when the regression technique is performed on the test data set.</li>
</ul>
<p><strong>Ensemble Regressor:-</strong> The goal of <strong>ensemble methods</strong> is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</p>
<p>&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong><span style="text-decoration: underline;">Random Forest Regressor</span></strong>:-A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if <cite>bootstrap=True</cite> (default).</p>
<p><span style="text-decoration: underline;">&nbsp;</span></p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:-</span></p>
<p><strong>n_estimators</strong> : <em>integer, optional (default=10)</em>.The number of trees in the forest.</p>
<p><strong>criterion</strong> : <em>string, optional (default=&rdquo;mse&rdquo;)</em></p>
<p>The function to measure the quality of a split. Supported criteria are &ldquo;mse&rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &ldquo;mae&rdquo; for the mean absolute error.</p>
<p>&nbsp;</p>
<p><strong>max_depth</strong> : <em>integer or None, optional (default=None)</em></p>
<p>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>
<p><strong>min_samples_split</strong> : <em>int, float, optional (default=2)</em></p>
<p>The minimum number of samples required to split an internal node:</p>
<ul>
    <li>If int, then consider min_samples_split as the minimum number.</li>
    <li>If float, then min_samples_split is a fraction and <em>ceil(min_samples_split * n_samples)</em> are the minimum number of samples for each split.</li>
</ul>
<p><strong>min_samples_leaf</strong> : <em>int, float, optional (default=1)</em></p>
<p>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</p>
<ul>
    <li>If int, then consider min<em>_</em>samples<em>_</em>leaf as the minimum number.</li>
    <li>If float, then <em>min_samples_leaf</em> is a fraction and <em>ceil(min_samples_leaf * n_samples)</em> are the minimum number of samples for each node.</li>
</ul>
<p>&nbsp;</p>
<p><strong>min_weight_fraction_leaf</strong> : <em>float, optional (default=0.)</em></p>
<p>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</p>
<p>&nbsp;</p>
<p><strong>max_features</strong> : <em>int, float, string or None, optional (default=&rdquo;auto&rdquo;)</em></p>
<p>The number of features to consider when looking for the best split:</p>
<ul>
    <li>If int, then consider <em>max_features</em> features at each split.</li>
    <li>If float, then <em>max_features</em> is a fraction and <em>int(max_features * n_features)</em> features are considered at each split.</li>
    <li>If &ldquo;auto&rdquo;, then <em>max_features=n_features</em>.</li>
    <li>If &ldquo;sqrt&rdquo;, then <em>max_features=sqrt(n_features)</em>.</li>
    <li>If &ldquo;log2&rdquo;, then <em>max_features=log2(n_features)</em>.</li>
    <li>If None, then <em>max_features=n_features</em>.</li>
</ul>
<p>&nbsp;Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.</p>
<p><strong>max_leaf_nodes</strong> : <em>int or None, optional (default=None)</em></p>
<p>Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</p>
<p><strong>min_impurity_decrease</strong> : <em>float, optional (default=0.)</em>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div>
    <ul>
        <li>N_t / N * (impurity - N_t_R / N_t * right_impurity</li>
        <li>&nbsp;- N_t_L / N_t * left_impurity)</li>
    </ul>
</div>
<p>where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.</p>
<p>N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.</p>
<p><strong>min_impurity_split</strong> : <em>float, (default=1e-7)</em></p>
<p>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</p>
<p><strong>bootstrap</strong> : <em>boolean, optional (default=True)</em></p>
<p>Whether bootstrap samples are used when building trees.</p>
<p><strong>oob_score</strong> : <em>bool, optional (default=False)</em></p>
<p>whether to use out-of-bag samples to estimate the R^2 on unseen data.</p>
<p><strong>n_jobs</strong> : <em>int or None, optional (default=None)</em></p>
<p>The number of jobs to run in parallel for both <em>fit</em> and <em>predict</em>.</p>
<p><strong>random_state</strong> : <em>int, RandomState instance or None, optional (default=None)</em></p>
<p>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by <em>np.random</em>.</p>
<p><strong>verbose</strong> : <em>int, optional (default=0)</em></p>
<p>Controls the verbosity when fitting and predicting.</p>
<p><strong>warm_start</strong> : <em>bool, optional (default=False)</em></p>
<p>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest</p>
<p><strong>&nbsp; </strong></p>
<p><strong>&nbsp;</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong><span style="text-decoration: underline;">Gradient Boost Regressor</span></strong><strong>:-</strong> Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.</p>
<p>The advantages of GBRT are:</p>
<ul>
    <li>Natural handling of data of mixed type (= heterogeneous features)</li>
    <li>Predictive power</li>
    <li>Robustness to outliers in output space (via robust loss functions)</li>
</ul>
<p>The disadvantages of GBRT are:</p>
<ul>
    <li>Scalability, due to the sequential nature of boosting it can hardly be parallelized.</li>
</ul>
<p><strong>&nbsp;</strong><span style="text-decoration: underline;">HYPERPARAMETERS:-</span></p>
<p><strong>loss</strong> <em>: </em>{{'{'}}&lsquo;ls&rsquo;, &lsquo;lad&rsquo;, &lsquo;huber&rsquo;, &lsquo;quantile&rsquo;}, optional (default=&rsquo;ls&rsquo;)</p>
<p>loss function to be optimized. &lsquo;ls&rsquo; refers to least squares regression. &lsquo;lad&rsquo; (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. &lsquo;huber&rsquo; is a combination of the two. &lsquo;quantile&rsquo; allows quantile regression.</p>
<p><strong>learning_rate</strong> : float, optional (default=0.1)</p>
<p>learning rate shrinks the contribution of each tree by <em>learning_rate</em>. There is a trade-off between learning_rate and n_estimators.</p>
<p><strong>n_estimators</strong> : int (default=100)</p>
<p>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</p>
<p><strong>subsample</strong> : float, optional (default=1.0)</p>
<p>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. <em>subsample</em> interacts with the parameter <em>n_estimators</em>. Choosing <em>subsample &lt; 1.0</em> leads to a reduction of variance and an increase in bias.</p>
<p><strong>criterion</strong> : string, optional (default=&rdquo;friedman_mse&rdquo;)</p>
<p>The function to measure the quality of a split. Supported criteria are &ldquo;friedman_mse&rdquo; for the mean squared error with improvement score by Friedman, &ldquo;mse&rdquo; for mean squared error, and &ldquo;mae&rdquo; for the mean absolute error. The default value of &ldquo;friedman_mse&rdquo; is generally the best as it can provide a better approximation in some cases.</p>
<p>&nbsp;</p>
<p><strong>min_samples_split</strong> <em>: </em>int, float, optional (default=2<em>)</em></p>
<p>The minimum number of samples required to split an internal node:</p>
<ul>
    <li>If int, then consider <em>min_samples_split</em> as the minimum number.</li>
    <li>If float, then <em>min_samples_split</em> is a fraction and <em>ceil(min_samples_split * n_samples)</em> are the minimum number of samples for each split.</li>
</ul>
<p><strong>min_samples_leaf</strong> : int, float, optional (default=1)</p>
<p>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</p>
<ul>
    <li>If int, then consider <em>min_samples_leaf</em> as the minimum number.</li>
    <li>If float, then <em>min_samples_leaf</em> is a fraction and <em>ceil(min_samples_leaf * n_samples)</em> are the minimum number of samples for each node..</li>
</ul>
<p><strong>min_weight_fraction_leaf</strong> : float, optional (default=0.)</p>
<p>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</p>
<p><strong>max_depth</strong> : integer, optional (default=3)</p>
<p>maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.</p>
<p><strong>min_impurity_decrease</strong> : float, optional (default=0.)</p>
<p>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div>
    <p>N_t / N * (impurity - N_t_R / N_t * right_impurity</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - N_t_L / N_t * left_impurity)</p>
</div>
<p>where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.</p>
<p>N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.</p>
<p><strong>min_impurity_split</strong> : float, (default=1e-7)</p>
<p>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</p>
<p><em>Deprecated since version 0.19: </em>min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead.</p>
<p><strong>init</strong> : estimator, optional (default=None)</p>
<p>An estimator object that is used to compute the initial predictions. init has to provide fit and predict. If None it uses loss.init_estimator.</p>
<p><strong>random_state</strong> : int, RandomState instance or None, optional (default=None)</p>
<p>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by <em>np.random</em>.</p>
<p><strong>max_features</strong> : int, float, string or None, optional (default=None)</p>
<p>The number of features to consider when looking for the best split:</p>
<ul>
    <li>If int, then consider <em>max_features</em> features at each split.</li>
    <li>If float, then <em>max_features</em> is a fraction and <em>int(max_features * n_features)</em> features are considered at each split.</li>
    <li>If &ldquo;auto&rdquo;, then <em>max_features=n_features</em>.</li>
    <li>If &ldquo;sqrt&rdquo;, then <em>max_features=sqrt(n_features)</em>.</li>
    <li>If &ldquo;log2&rdquo;, then <em>max_features=log2(n_features)</em>.</li>
    <li>If None, then <em>max_features=n_features</em>.</li>
</ul>
<p>Choosing <em>max_features &lt; n_features</em> leads to a reduction of variance and an increase in bias.</p>
<p>Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.</p>
<p><strong>alpha</strong> : float (default=0.9)</p>
<p>The alpha-quantile of the huber loss function and the quantile loss function. Only if loss='huber' or loss='quantile'.</p>
<p><strong>verbose</strong> : int, default: 0</p>
<p>Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree.</p>
<p><strong>max_leaf_nodes</strong> : int or None, optional (default=None)</p>
<p>Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</p>
<p><strong>warm_start</strong> : bool, default: False</p>
<p>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution.</p>
<p><strong>presort</strong> <em>: </em>bool or &lsquo;auto&rsquo;, optional (default=&rsquo;auto&rsquo;)</p>
<p>Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error.</p>
<p><strong>validation_fraction</strong> : float, optional, default 0.1</p>
<p>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if n_iter_no_change is set to an integer.<em>.</em></p>
<p><strong>n_iter_no_change</strong> : int, default None</p>
<p>n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations.</p>
<p><strong>tol</strong> : float, optional, default 1e-4</p>
<p>Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops.</p>
<p><span style="text-decoration: underline;">&nbsp;</span></p>
<p>&nbsp;</p>
<p align="center"><strong><em><span style="text-decoration: underline;">Classification</span></em></strong></p>
<p>Types of multinomial classifications to discuss:</p>
<p>&nbsp;</p>
<ul>
    <li>Decision Trees</li>
    <li>Random Forests</li>
    <li>Support Vector Machines</li>
    <li>KNN</li>
    <li>Gaussian Naive Bayes</li>
    <li>Stochastic gradient descent</li>
</ul>
<p>&nbsp;</p>
<p><strong><em><span style="text-decoration: underline;">Decision Tree:</span></em></strong> Decision Tree classifier is a classifier that has a structure similar to that of a tree. The nodes represent the feature on which the split is done, the branches represent the decision rule and the leaf nodes represent the outcome. The flow-chart like structure is very intuitive in terms of decision-making process. The algorithm can be summarized as follows: The best feature is chosen to make a split on. This divides the dataset into smaller subsets. The tree is recursively built until stopping criterion is reached. The stopping criterion can be any of the following:</p>
<ul>
    <li>All the remaining data belong to the same feature</li>
    <li>No more remaining feature to split the dataset on</li>
    <li>No more datapoints remaining to assign.</li>
</ul>
<p>The best feature for splitting is decided based on the rank or score given to each feature at each stage. The feature with the maximum score, is the one which acts as the node. The most popular selection measures are Information Gain, Gain Ratio, and Gini Index.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>criterion </strong>(<em>optional</em>, default value = &lsquo;gini&rsquo;): Takes String inputs. It defines the selection measure on which the split is made. &ldquo;gini&rdquo; is chosen for the Gini Index and &ldquo;entropy&rdquo; for the Information Gain selection measure.</li>
    <li><strong>splitter </strong>(<em>optional</em>, default value = &lsquo;best&rsquo;): Takes either &lsquo;best&rdquo; or &ldquo;random&rdquo; as inputs. &ldquo;best&rdquo; selects the best split while &ldquo;random&rdquo; selects the best random split.</li>
    <li><strong>max_depth </strong>(<em>optional, </em>default value = None): It takes either an int value or &lsquo;None&rsquo; as input. As the name suggests, it defines the maximum depth of the tree desired while building the tree. For None, the leaves are expanded until all leaves belong to the same subcategory or until all leaves contain less than min_samples_split samples.</li>
    <li><strong>min_samples_split </strong>(<em>optional, </em>default =2): Takes int or float values as inputs. It defines the minimum number of samples requires to split an internal node.</li>
    <li><strong>min_samples_leaf </strong>(<em>optional, </em>default value = 1): Takes int or float values as inputs. It defines the minimum number of samples requires to be at a leaf node.</li>
    <li><strong>min_weight_fraction_leaf</strong> (<em>optional, </em>default value = 0): It takes float values as inputs. It defines the minimum weighted fraction of the sum total of weights of all the inputs samples required to be at a leaf node.</li>
    <li><strong>max-features </strong>(<em>optional, </em>default value = None): It takes either int or float or string or None as input. It defines the number of features that are considered when making the best split.</li>
    <li><strong>random_state </strong>(<em>optional</em>, default value = None): It specifies the seed for the random number generator which is used to shuffle the data. It takes integer value, by default set to None.</li>
    <li><strong>max_leaf_nodes </strong>(<em>optional</em>, default value = None): It takes either int value or None. It allows the algorithm to grow the tree with max_leaf_nodes in best-first fashion.</li>
    <li><strong>min_impurity_decrease </strong>(<em>optional</em>, default value = None): It takes either a float value or None. If the impurity is greater than or equal to this value, then the algorithm will split a node.</li>
    <li><strong>min_impurity_split </strong>(<em>optional</em>, default value = 1e-7): It takes either a float value. If the impurity at a stage is above this threshold value, then only the node will be split further, otherwise it will be considered as a leaf node.</li>
    <li><strong>class_weight </strong>(default value = None): Accepts a dictionary, list of dictionaries, &lsquo;balanced&rsquo; or None as inputs. Default is set to None. If not set, all the classes are weighted equally, i.e., their weights are considered equal to 1.</li>
    <li><strong>presort </strong>(<em>optional</em>, default value = False): It takes Boolean value as input. As the name suggests, it determines if the dataset should be pre-sorted or not. For smaller datasets or for datasets with restricted depths, setting this to True might speed up the tree building process, whereas the opposite holds true for larger datasets.</li>
</ul>
<p><span style="text-decoration: underline;">&nbsp;</span></p>
<p><span style="text-decoration: underline;">OUTPUT: </span>The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>d_tree_train_score</strong>: The accuracy score achieved when the classification technique is performed on the training data set.</li>
    <li><strong>d_tree_test_score</strong>: The accuracy score achieved when the classification technique is performed on the test data set.</li>
</ul>
<p>&nbsp;</p>
<p><strong>Random Forest:-</strong> Random forest or Random decision forests are popular&nbsp; ensemble methods that can be used to build predictive models for both classification and regression problems.</p>
<ul>
    <li>In general, the more trees in the forest the more robust the forest looks like. In the same way in the random forest classifier, the higher the number of trees in the forest gives the high accuracy results.</li>
    <li>Random forests combine the predictions of multiple decision trees. In constructing a decision tree, the dataset is repeatedly divided into subtrees, guided by the best combination of variables.</li>
</ul>
<p><strong><span style="text-decoration: underline;">Advantages:-</span></strong></p>
<ul>
    <li>Handles both classification and Regression.</li>
    <li>Won&rsquo;t overfit the model.</li>
    <li>Handles large data set with higher dimensionality.</li>
    <li>Handles the missing data and maintains accuracy for missing data.</li>
</ul>
<p><strong><span style="text-decoration: underline;">Disadvantages:-</span></strong></p>
<ul>
    <li>It cannot predict beyond the data in the training model.</li>
</ul>
<p><strong><span style="text-decoration: underline;">Uses:- </span></strong>Banking, Medicine, Finance, E-commerce.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="text-decoration: underline;">&nbsp;HYPERPARAMETERS:-</span></p>
<ul>
    <li>n_estimators : integer, optional (default=10)The number of trees in the forest.</li>
    <li>criterion : string, optional (default=&rdquo;gini&rdquo;).The function to measure the quality of a split. Supported criteria are &ldquo;gini&rdquo; for the Gini impurity and &ldquo;entropy&rdquo; for the information gain.</li>
    <li>max_depth : integer or None, optional (default=None).The maximum depth of the tree.</li>
    <li>min_samples_split : int, float, optional (default=2)</li>
    <li>min_samples_leaf : int, float, optional (default=1)The minimum number of samples required to be at a leaf node.</li>
    <li>min_weight_fraction_leaf : float, optional (default=0.)The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.</li>
    <li>max_features : int, float, string or None, optional (default=&rdquo;auto&rdquo;)The number of features to consider when looking for the best split:</li>
</ul>
<p>&uuml;&nbsp; If int, then consider max_features features at each split.</p>
<p>&uuml;&nbsp; If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.</p>
<p>&uuml;&nbsp; If &ldquo;auto&rdquo;, then max_features=sqrt(n_features).</p>
<p>&uuml;&nbsp; If &ldquo;sqrt&rdquo;, then max_features=sqrt(n_features) (same as &ldquo;auto&rdquo;).</p>
<p>&uuml;&nbsp; If &ldquo;log2&rdquo;, then max_features=log2(n_features).</p>
<p>&uuml;&nbsp; If None, then max_features=n_features.</p>
<ul>
    <li>min_impurity_split : float, (default=1e-7)Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</li>
    <li>bootstrap : boolean, optional (default=True)Whether bootstrap samples are used when building trees.</li>
    <li>oob_score : bool (default=False).Whether to use out-of-bag samples to estimate the generalization accuracy.</li>
    <li>n_jobs : int or None, optional (default=None)The number of jobs to run in parallel for both fit and predict.</li>
    <li>verbose : int, optional (default=0)Controls the verbosity when fitting and predicting.</li>
    <li>warm_start : bool, optional (default=False)When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.</li>
    <li>class_weight : dict, list of dicts, &ldquo;balanced&rdquo;, &ldquo;balanced_subsample&rdquo; or None, optional (default=None)Weights associated with classes in the form {{'{'}}class_label: weight}.</li>
</ul>
<p>&nbsp;</p>
<p><strong><em><span style="text-decoration: underline;">SVM Classifier:</span></em></strong>The idea behind Support Vector Machine Classifier is to create a hyperplane that separates the datapoints belonging to different classes in such a way that minimum error is encountered when new datapoints are classified in future. The goal is to generalize the classifier for all the future data points. This can be done by maximizing the margin which is the distance of the separating hyperplane from the nearest training data point of any class. These deciding data points are called the support vectors. SVM classifier searches for the maximum marginal hyperplane with respect to all the classes to minimize the generalized error.</p>
<p>To deal with non-linear dataset, SVM uses kernel trick which transforms the input space into a higher dimensional space where the dataset becomes linearly separable. This is done using a kernel function.&nbsp;</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>C </strong>(<em>optional</em>, default value = 1.0): Takes float values as input. It defines the regularization term of the error which is used to avoid overfitting.</li>
    <li><strong>kernel </strong>(<em>optional</em>, default value = &lsquo;rbf&rsquo;): Takes any of {{'{'}}&lsquo;linear&rsquo;, &lsquo;poly&rsquo;, &lsquo;rbf&rsquo;, &lsquo;sigmoid&rsquo;, &lsquo;precomputed&rsquo;} or a callable as input. These are different kernel types used in the algorithm.</li>
    <li><strong>degree </strong>(<em>optional, </em>default value = 3): If the kernel chosen is anything other than &lsquo;poly&rsquo;, this hyperparameter is ignored. For &lsquo;poly&rsquo; kernel type, it defines the degree of the polynomial kernel function.</li>
    <li><strong>gamma </strong>(<em>optional, </em>default = &lsquo;auto&rsquo;): Takes float values as input. It is the kernel coefficient for &lsquo;rbf&rsquo;, &lsquo;poly&rsquo; and &lsquo;sigmoid&rsquo; kernel types.</li>
    <li><strong>coef0 </strong>(<em>optional, </em>default value = 0.0): Takes float values as inputs. It is the independent term in kernel function.</li>
    <li><strong>shrinking</strong> (<em>optional, </em>default value = True): It takes Boolean values describing whether to use shrinking heuristic or not. It reduces the training time by trying to identify and remove some of the bounded elements, so that only a smaller optimization problem needs to be solved.</li>
    <li><strong>probability </strong>(<em>optional, </em>default value = False): It takes a Boolean value as input. If set to tru, it enables probability estimates.</li>
    <li><strong>tol </strong>(<em>optional</em>, default value = 1e-3): It specifies the precision of the solution. Takes positive float values.</li>
    <li><strong>cache_size </strong>(<em>optional</em>, default value = 200): It takes float values as input. It specifies the size of the cache in MB.</li>
    <li><strong>class_weight </strong>(default value = None): Accepts a dictionary, list of dictionaries, &lsquo;balanced&rsquo; or None as inputs. Default is set to None. If not set, all the classes are weighted equally, i.e., their weights are considered equal to 1.</li>
    <li><strong>min_impurity_split </strong>(<em>optional</em>, default value = 1e-7): It takes either a float value. If the impurity at a stage is above this threshold value, then only the node will be split further, otherwise it will be considered as a leaf node.</li>
    <li><strong>class_weight </strong>(default value = None): Accepts a dictionary, list of dictionaries, &lsquo;balanced&rsquo; or None as inputs. Default is set to None. If not set, all the classes are weighted equally, i.e., their weights are considered equal to 1.</li>
    <li><strong>random_state </strong>(<em>optional</em>, default value = None): It specifies the seed for the random number generator which is used to shuffle the data. It takes integer value, by default set to None.</li>
    <li><strong>verbose </strong>(default value = False): It takes Boolean value as input. If set to True, it will enable verbose output.</li>
    <li><strong>max_iter </strong>(<em>optional</em>, default value = -1): It is the maximum number of iterations for the solver used. It takes integer inputs. -1 is used for no limit.</li>
    <li><strong>decision_function_shape </strong>(default value = &lsquo;ovr&rsquo;): It takes &lsquo;ovo&rsquo; for one-vs-one decision function or &lsquo;ovr&rsquo; for one-vs-rest decision function.</li>
</ul>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>svm_train_score</strong>: The accuracy score achieved when the regression technique is performed on the training data set.</li>
    <li><strong>svm_test_score</strong>: The accuracy score achieved when the regression technique is performed on the test data set.</li>
</ul>
<p>&nbsp;</p>
<p align="center"><span style="text-decoration: underline;">&nbsp;</span></p>
<p><strong><em><span style="text-decoration: underline;">K- Nearest Neighbor:</span></em></strong> KNN is a non-parametric (no assumption of underlying distribution) and &ldquo;lazy&rdquo; learning (it does not generate a trained model; it predicts the class for new data points on the fly, considering all the previous data points) algorithm. In simple terms, the algorithm works as follows:</p>
<ul>
    <li>Define a positive integer K</li>
    <li>Get the new data point whose respective class needs to be predicted</li>
    <li>Select the K data points from the already existing data set, which are the closest to the new sample data point.</li>
    <li>Find the most common classification of these K data points. The class that is the most frequent is assigned as the predicted class for the new sample data point.</li>
</ul>
<p>KNN is easy to understand and since it makes no prior assumption about the underlying distribution, it serves particularly well for nonlinear data. But since, the algorithm stores all the previous data points and generates no model for future prediction (i.e., it performs classification just in time by calculating similarity between the new data point and all the previous data points), the prediction stage might be slow and computationally expensive.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>n_neighbors </strong>(<em>optional</em>, default value = 5): Number of neighbors, i.e., K in the above described algorithm. Takes integer values as input.</li>
    <li><strong>weights </strong>(<em>optional</em>, default value = &lsquo;uniform&rsquo;): It takes any of the following as inputs {{'{'}}&lsquo;uniform&rsquo;, &lsquo;distance&rsquo; or a [callable]}. &lsquo;uniform&rsquo; gives equal importance/weights to all the neighbors in consideration, &lsquo;distance&rsquo; gives more importance to closer neighbors than those which are farther away, while, [callable] is a user defined function which would define how the importance/weights of the neighbors varies.</li>
    <li><strong>algorithm </strong>(<em>optional, </em>default value = &lsquo;auto&rsquo;): It takes any of the following as inputs {{'{'}}&lsquo;auto&rsquo; &lsquo;ball_tree&rsquo;, &lsquo;kd_tree&rsquo;, &lsquo;brute&rsquo;}. While &lsquo;ball_tree&rsquo;, &lsquo;kd_tree&rsquo; and &lsquo;brute&rsquo; algorithms will use their corresponding namesake algorithms, &lsquo;auto&rsquo; will try to find the best fit to the given dataset.</li>
    <li><strong>leaf_size </strong>(<em>optional, </em>default =30): It is applicable only for &lsquo;ball_tree&rsquo; and &lsquo;kd_tree&rsquo; algorithms. It defines the size of the leaf. Larger size will result faster initial indexing of the tree structure, but target classification would be delayed.</li>
    <li><strong>p </strong>(<em>optional, </em>default value = 2): It defines the power for the Minkowski metric. The Minkowski distance becomes the same as Manhattan distance when p = 1, and becomes equal to Euclidean distance when p=2.</li>
    <li><strong>metric </strong>(default value = &lsquo;minkowski&rsquo;): It takes either a string (e.g. &lsquo;minkowski&rsquo;, &lsquo;euclidean&rsquo;, &lsquo;manhattan&rsquo;, etc.) or a [callable] as input.</li>
    <li><strong>metric_params </strong>(<em>optional, </em>default value = None): It takes a dictionary as input. Used to pass additional fine-tuning parameters.</li>
    <li><strong>n_jobs </strong>(<em>optional, </em>default value = None): It specifies the how many parallel processors will work on the problem at hand. &lsquo;None&rsquo; would mean only 1 processor is working while -1 means all the processors are used.</li>
</ul>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>knn_train_score</strong>: The accuracy score achieved when the classification technique is performed on the training data set.</li>
    <li><strong>knn_test_score</strong>: The accuracy score achieved when the classification technique is performed on the test data set.</li>
</ul>
<p><strong><em><span style="text-decoration: underline;">Na&iuml;ve Bayes Classifier:</span></em></strong> Bayes Classifier is based on Bayes Theorem which can be mathematically represented as follows</p>
<p><strong><img src="assets/images/regression_buyerclassify.png" alt="" /></strong></p>
<p>Class Prior Probability: The probability of class &lsquo;c&rsquo; being the true class despite of what the data(x) is.</p>
<p>Predictor/Prior Probability: The probability of the data(x) regardless of the class.</p>
<p>Likelihood: The probability of the data(x) given the class(c).</p>
<p>Posterior Probability: The probability of the class(c) being the true class given the data(x)</p>
<p>The reason why this classification technique is called the Na&iuml;ve is because it assumes that the effect of one feature in determining the class of the given data point is independent of all the other features.</p>
<p>The major shortcomings of Na&iuml;ve Bayes are:</p>
<ul>
    <li>This assumption that the features affect the class determination process for the new data point independently, can lead to a very bad decision model.</li>
    <li>Calculating the probabilities for features which are continuous in nature by traditional methods of frequency counts is not possible. Binning is usually opted to overcome this hurdle.</li>
</ul>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>priors </strong>(default value = None): It accepts arrays of size equal to the number of classes the dataset needs to be classified into. It defines the prior probabilities of the data at hand. If set to None, the algorithm itself will adjust the prior probabilities for the classes.</li>
    <li><strong>var_smoothing </strong>(<em>optional</em>, default value = 1e-09): It accepts float values. It&rsquo;s a value added for calculation stability.</li>
</ul>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>gnb_train_score</strong>: The accuracy score achieved when the regression technique is performed on the training data set.</li>
    <li><strong>gnb_test_score</strong>: The accuracy score achieved when the regression technique is performed on the test data set.</li>
</ul>
<p>&nbsp;</p>
<p class="bans"><strong>Stochastic Gradient Descent:-</strong> Stochastic gradient descent (often shortened in SGD) is a gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions.</p>
<ul>
    <li>It is a very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machine and logistic regression.</li>
    <li>It is called <strong>stochastic</strong> because samples are selected randomly (or shuffled) instead of as a single group.</li>
</ul>
<p>&nbsp;</p>
<p>The advantages of Stochastic Gradient Descent are:-</p>
<ul>
    <li>Efficiency.</li>
    <li>Ease of implementation (lots of opportunities for code tuning)</li>
</ul>
<p>&nbsp;</p>
<p>The disadvantages of Stochastic Gradient Descent include:</p>
<ul>
    <li>SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.</li>
    <li>SGD is sensitive to feature scaling.</li>
</ul>
<p><strong><span style="text-decoration: underline;">HYPERPARAMETERS</span></strong></p>
<ul>
    <li>loss : str, default: &lsquo;hinge&rsquo;.The loss function to be used. Defaults to &lsquo;hinge&rsquo;, which gives a linear SVM.</li>
    <li>penalty : str, &lsquo;none&rsquo;, &lsquo;l2&rsquo;, &lsquo;l1&rsquo;, or &lsquo;elasticnet&rsquo;</li>
</ul>
<p>Defaults to &lsquo;l2&rsquo; which is the standard regularizer for linear SVM models. &lsquo;l1&rsquo; and &lsquo;elasticnet&rsquo; might bring sparsity to the model (feature selection) not achievable with &lsquo;l2&rsquo;.</p>
<ul>
    <li>alpha : float.Defaults to 0.0001 Also used to compute learning_rate when set to &lsquo;optimal&rsquo;.</li>
</ul>
<p>l1_ratio : float</p>
<p>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</p>
<ul>
    <li>fit_intercept : bool.Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.</li>
    <li>max_iter : int, optional</li>
</ul>
<p>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</p>
<ul>
    <li>tol : float or None, optional</li>
</ul>
<p>The stopping criterion. If it is not None, the iterations will stop when (loss &gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</p>
<ul>
    <li>shuffle : bool, optional.Defaults to True.</li>
    <li>verbose : integer, optional</li>
    <li>epsilon : float</li>
    <li>n_jobs : int or None, optional (default=None)</li>
    <li>random_state : int, RandomState instance or None, optional (default=None)</li>
    <li>learning_rate : string, optional</li>
</ul>
<p>The learning rate schedule:</p>
<p>&lsquo;<strong>constant&rsquo;</strong>:</p>
<p>eta = eta0</p>
<p>&lsquo;<strong>optimal</strong>&rsquo;: [default]</p>
<p>eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.</p>
<p><strong>&lsquo;invscaling&rsquo;</strong>:</p>
<p>eta = eta0 / pow(t, power_t)</p>
<p><strong>&lsquo;adaptive&rsquo;</strong>:</p>
<p>eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.</p>
<ul>
    <li>eta0 : double</li>
</ul>
<p>The initial learning rate for the &lsquo;constant&rsquo;, &lsquo;invscaling&rsquo; or &lsquo;adaptive&rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &lsquo;optimal&rsquo;.</p>
<ul>
    <li>power_t : double</li>
</ul>
<p>The exponent for inverse scaling learning rate [default 0.5].</p>
<ul>
    <li>early_stopping : bool, default=False</li>
    <li>validation_fraction : float, default=0.1</li>
</ul>
<p>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</p>
<ul>
    <li>n_iter_no_change : int, default=5</li>
</ul>
<p>Number of iterations with no improvement to wait before early stopping.</p>
<ul>
    <li>class_weight : dict, {{'{'}}class_label: weight} or &ldquo;balanced&rdquo; or None, optional</li>
    <li>warm_start : bool, optional</li>
</ul>
<p>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.</p>
<ul>
    <li>average : bool or int, optional</li>
</ul>
<p>When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.</p>
<ul>
    <li>n_iter : int, optional. Defaults to None.</li>
</ul>
<p>&nbsp;</p>
<p><strong>Clustering:-</strong></p>
<ul>
    <li>Affinity Propagation Clustering</li>
    <li>K-means clustering</li>
</ul>
<p>&nbsp;</p>
<p><strong><em><span style="text-decoration: underline;">Affinity Propagation Clustering:</span></em></strong>&nbsp; Affinity Propagation is a clustering algorithm that works by randomly choosing an initial set of datapoints that may represent different clusters and iteratively refining the set. These data points are known as exemplars. The algorithm considers all the points as potential exemplars. The main advantage of Affinity propagation is that there is no need to define the number of clusters at the beginning of the algorithm.</p>
<p>Two inputs are provided at the start of the algorithm, usually represented in the form of a matrix:</p>
<ul>
    <li><em>Similarities</em>: Similarities between the data points representing how similar any data points are.</li>
    <li><em>Preferences</em>: Preferences of each data point representing how suitable it is to be an exemplar.</li>
</ul>
<p>These two inputs may be represented through a single matrix where the diagonal elements represent preferences.</p>
<p>Let &lsquo;i&rsquo; represent a data point and &lsquo;k&rsquo; represent a candidate exemplar. The algorithm works by passing messages. In each iteration, two message-passing steps take place:</p>
<ul>
    <li>Calculating responsibilities [ r(i, k) ]: Reflects how well suited &lsquo;k&rsquo; is to act as the exemplar for &lsquo;i&rsquo; considering other potential exemplars for &lsquo;i&rsquo;.</li>
    <li>Calculating availabilities [ a(i, k) ]: Reflects how appropriate it would be for &lsquo;i&rsquo;&nbsp; to choose &lsquo;k&rsquo; as the exemplar considering all the other data points that support &lsquo;k&rsquo; as an exemplar.</li>
</ul>
<p>We can have different stopping criteria to terminate the procedure such as a maximum number of iterations being reached or changes in values falling below a certain threshold.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:</span></p>
<ul>
    <li><strong>damping </strong>(<em>optional, </em>default value = 0.5): Takes float values between 0.5 to 1 as input. As new data points are added to the dataset, this damping factor decides the weights for the datapoints and to what extent the current value is maintained relative to these incoming datapoints.</li>
    <li><strong>affinity </strong>(<em>optional</em>, default value = &lsquo;euclidean&rsquo;): Takes string or a callable as input. It defines the distance measuring metric used to find how dissimilar (and hence how similar) two clusters are. It can be &ldquo;euclidean&rdquo; or &ldquo;precomputed&rdquo;.</li>
    <li><strong>max_iter </strong>(<em>optional, </em>default value = 200): It is the maximum number of iterations used. It takes integer inputs.</li>
    <li><strong>convergence_iter </strong>(<em>optional, </em>default = 15): Takes integer values as input. If the number of estimated clusters does not change for these many iterations, the algorithm will reach a stopping criterion.</li>
    <li><strong>copy </strong>(<em>optional,</em> default = True): If set, the input data will be copied.</li>
    <li><strong>preference</strong> (<em>optional, </em>default = None): It takes either an array-like value, or float. It defines the preference for each point to be an exemplar. The points with higher preference will have higher chance to become an exemplar.</li>
    <li><strong>verbose </strong>(<em>optional, </em>default = False): Whether to log detailed information or not.</li>
</ul>
<p>&nbsp;</p>
<p><span style="text-decoration: underline;">OUTPUT:</span> The following outputs are stored in a dictionary and dumped into a json object which is returned as output</p>
<ul>
    <li><strong>hyperParams</strong>: The algorithm returns the set of fined tuned hyperparameters (as discussed above) in dictionary format.</li>
    <li><strong>AffinityPropagationClustering_Score</strong>: The best accuracy score achieved when the clustering technique is performed on the data set at hand. The scoring metric chosen is &ldquo;Homogeneity&rdquo;.</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><strong>K-Means Clustering:-</strong> K-means clustering is one of the most widely used unsupervised machine learning algorithms that forms clusters of data based on the similarity between data instances.</p>
<ul>
    <li>For this particular algorithm to work, the number of clusters has to be defined beforehand. The K in the K-means refers to the number of clusters.</li>
    <li>The K-means algorithm starts by randomly choosing a centroid value for each cluster. After that the algorithm iteratively performs three steps:</li>
</ul>
<p><strong>&nbsp;</strong></p>
<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>Find the Euclidean distance between each data instance and centroids of all the clusters</p>
<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>Assign the data instances to the cluster of the centroid with nearest distance</p>
<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong>Calculate new centroid values based on the mean values of the coordinates of all&nbsp;&nbsp;&nbsp; the&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data instances from the corresponding cluster.</p>
<p><span style="text-decoration: underline;">HYPERPARAMETERS:-</span></p>
<ul>
    <li><strong>n_clusters</strong> : <em>int, optional, default: 8</em></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The number of clusters to form as well as the number of centroids to generate.</p>
<ul>
    <li><strong>init</strong> : <em>{{'{'}}&lsquo;</em>k-means++&rsquo;, &lsquo;random&rsquo; or an ndarray<em>}</em></li>
</ul>
<p>Method for initialization, defaults to &lsquo;k-means++&rsquo;: <strong>&lsquo;k-means++&rsquo;</strong> : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence</p>
<p><strong>&lsquo;random&rsquo;</strong>: choose k observations (rows) at random from data for the initial centroids.</p>
<p>If an <strong>ndarray </strong>is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</p>
<ul>
    <li><strong>n_init</strong> : int, default: 10</li>
</ul>
<p>Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.</p>
<ul>
    <li><strong>max_iter</strong> : int, default: 300</li>
</ul>
<p>Maximum number of iterations of the k-means algorithm for a single run.</p>
<ul>
    <li><strong>tol</strong> : <em>float, default: 1e-4</em></li>
</ul>
<p>Relative tolerance with regards to inertia to declare convergence</p>
<ul>
    <li><strong>precompute_distances</strong> : <em>{{'{'}}&lsquo;</em>auto&rsquo;, True, False<em>}</em></li>
</ul>
<p>Precompute distances (faster but takes more memory).</p>
<p>&lsquo;auto&rsquo; : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision.</p>
<p>True : always precompute distances</p>
<p>False : never precompute distances</p>
<ul>
    <li><strong>verbose</strong> : int, default 0</li>
</ul>
<p>Verbosity mode.</p>
<ul>
    <li><strong>random_state</strong> : int, RandomState instance or None (default<em>)</em></li>
</ul>
<p>Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.</p>
<ul>
    <li><strong>copy_x</strong> : boolean, optional</li>
</ul>
<p>When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown.</p>
<ul>
    <li><strong>n_jobs</strong> : int or None, optional (default=None)</li>
</ul>
<p>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.None means 1 unless in a <strong><a title="(in joblib v0.13.0)" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend">joblib.parallel_backend</a></strong> context. -1 means using all processors.</p>
<ul>
    <li><strong>algorithm</strong> : <em>&ldquo;</em>auto&rdquo;, &ldquo;full&rdquo; or &ldquo;elkan&rdquo;, default=&rdquo;auto&rdquo;</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; K-means algorithm to use. The classical EM-style algorithm is &ldquo;full&rdquo;.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The &ldquo;elkan&rdquo;&nbsp; variation is more efficient by using the triangle inequality, but&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; currently doesn&rsquo;t support sparse data. &ldquo;auto&rdquo; chooses &ldquo;elkan&rdquo; for dense</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data and &ldquo;full&rdquo; for sparse data.</p>
<p>&nbsp;</p>
</div>